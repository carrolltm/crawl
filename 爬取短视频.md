---
typora-root-url: 爬虫\images
typora-copy-images-to: 爬虫\images
---

# 爬取短视频

静态网页：request请求的就是当前url

动态网页：需要进行抓包处理。

判断条件

使用查看网页源代码：页面当中的数据在网页源代码中有，那就是静态的，否则的就是需要动态抓包处理。

# 好看视频

参考网站

教学

https://www.bilibili.com/video/BV1fJ411J719?from=search&seid=15740661422866602053

步骤

看是否动态网页 检查源代码和网页有些话是否出现，出现在源代码中就是静态页面

![image-20210202221425603](/image-20210202221425603.png)

判断哪个包传的。

因此我们需要抓包，包地址和你正常访问地址很有可能不一样。

![image-20210202221623402](/image-20210202221623402.png)

**微博好像是静态页面**

## 单页面

对于请求头

User-Agent

![image-20210202222341417](/image-20210202222341417.png)

cookie

![image-20210202222516343](/image-20210202222516343.png)

![image-20210202223720804](/image-20210202223720804.png)

数据格式是unicode编码

数据解析

视频数据需要：获得title

![image-20210202224934898](/image-20210202224934898.png)

得到的json数据

![image-20210202225111592](/image-20210202225111592.png)

如何获得视频尾缀

直接拼接

```python
import requests
import json
base_url='https://haokan.baidu.com/videoui/api/videorec?tab=gaoxiao&act=pcFeed&pd=pc&num=20&shuaxin_id=1612276740204'
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',
    'Cookie': 'BIDUPSID=8303CDB62079598D55280B09241EC6A5; PSTM=1609205633; BAIDUID=8303CDB62079598D785D47E1C310AF5F:FG=1; ab_jid=e8f37dcb0d84ebde578eaeb4c81de8c0ef8e; ab_jid_BFESS=e8f37dcb0d84ebde578eaeb4c81de8c0ef8e; BAIDUID_BFESS=8303CDB62079598D785D47E1C310AF5F:FG=1; BDUSS=VkM29seVBtNW9BY35hdk80YzV4MXIyaS1UOWJPMnFnU2lmZ0tjak05TlZreDVnRVFBQUFBJCQAAAAAAAAAAAEAAADGgNJFysXLrsH3xOqy0NK5AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFUG919VBvdfTE; BDUSS_BFESS=VkM29seVBtNW9BY35hdk80YzV4MXIyaS1UOWJPMnFnU2lmZ0tjak05TlZreDVnRVFBQUFBJCQAAAAAAAAAAAEAAADGgNJFysXLrsH3xOqy0NK5AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFUG919VBvdfTE; H_PS_PSSID=33425_33581_33257_33273_31660_33594_33339_26350_33264; BA_HECTOR=85010120a4058h81331g1ilep0r; __yjs_duid=1_f17eb43300edc3f1cb0b26e84be486ad1612274351261; ab_sr=1.0.0_M2NkNDA5MTRmZWM0MDlmMzczMmQ2MmNiM2QxZGE2ZDcwYWE1MWE5MmM5YmY1YTc5YTFmYWMxYjdlNzNiM2MzYmRhYjg4M2MzMjM4ZTc1YjQ0NGFmYjliMjk4ZDU0YWM4'
}
# 发送请求
response = requests.get(base_url, headers=headers)
data = response.text
# print(data)
# 数据转换 把json字符串转换为python可以交换的类型
import json
json_data = json.loads(data)

# 数据解析  看你需要什么数据
# 一层层获得数据 网站json.cn可视化json
#  解析title，视频数据url类型（猜测）
data_list = json_data['data']['response']['videos']
# 遍历列表
for data_item in data_list:
    # print(data_item)
    #  视频文件名字 需要自己加尾缀
    video_title=data_item['title']+'.mp4' 
    #  视频存在位置
    video_Url = data_item['play_url']
    # print(video_title)
    print('正在下载',video_title)
    #  再次发送具体视频URL地址 得到的是二进制数据，需要后面加上一个content
    video_data = requests.get(video_Url,headers=headers).content

    #  保存在文件夹videos,wb方式写入，二进制方式
    with open('videos\\'+video_title,'wb') as f:
        f.write(video_data)
        print('下载完成...\n')


```

以上是单页面爬取

注意：

.content是是因为获得视频url得到的数据（图像,音频，视频）。

requests的请求返回的是一个对象

.text是文本数据

## 多页面爬取

如何实行翻页

看着network，你往下翻，看数据包是否变化

![image-20210202231904925](/image-20210202231904925.png)

![image-20210202231932375](/image-20210202231932375.png)

这是参数变化

这里翻页的包的请求参数没有变化。

**这是相同的URL的地址，返回的数据不一样。**这里是没有变化，需要注意的点。

有些是会变化的。

所以构建for循环就行了

![image-20210202232736552](/image-20210202232736552.png)

因为这个URL是没有变化的。

## 注意

b站不可以，因为是有参数加密。

# b站视频

参考视频

https://www.bilibili.com/video/BV1Fy4y1D7XS

网页源代码有些东西有，这是静态页面的吗？

这是视频流数据

b站视频画面和音频数据是分开的，意思就是视频时无声的。



它是在playinfo找到baseurl中找到视频地址。

但是没有声音。







微博

https://weibo.com/tv/show

这个类似于好看视频





# 爆米花视频

只是实现单页面模式，但是里面post_form中的

![image-20210203205339810](/image-20210203205339810.png)

需要自己调一次获得的视频个数，**但是这个搞笑区效果不好，暂时不打算继续下去，但是代码已经成功实现能成功爬取视频，只是多页面等操作看后面搞不搞把。**

跟好看视频是一样的模式

https://www.baomihua.com/funny

![image-20210203151115255](/image-20210203151115255.png)

地址在这里面的link，然后点击之后跳转到另外一个页面然后还可以获得视频url地址。

![image-20210203151329631](/image-20210203151329631.png)



然后就得到地址。

上面是post请求

https://www.bilibili.com/video/BV1Ub411B7aM?from=search&seid=739901640749625509

它前面得到Playid,的后面是后一个链接需要post的数据。

![image-20210203202616090](/image-20210203202616090.png)

[解析url得到后面的参数](https://blog.csdn.net/qq_41236493/article/details/96470760)

最后一个得到json得到mp4

![image-20210203203126181](/image-20210203203126181.png)



# IP代理池

第一次尝试：

[参考1：89免费代理已模仿成功](https://blog.csdn.net/weixin_49738000/article/details/112344463)

[参考2：没实现](https://blog.csdn.net/weixin_44613063/article/details/102538757)

[这个没实现，但感觉比第二个可靠，使用了数据库](https://blog.csdn.net/stephen_curry300/article/details/108952806)

[b站教你构建快代理代理池](https://www.bilibili.com/video/BV1RJ411Y7nm?t=4536)

实际运用到代理中出现的问题

![image-20210203222814234](/image-20210203222814234.png)

[解决办法参考](https://blog.csdn.net/qq_19294857/article/details/99653889)

目前快代理的高匿名代理可以使用了，

目标测试网址是百度

# 微博爬取

这貌似是静态页面，这个有视频的小栏的在class这个类里面

![image-20210204110233369](/image-20210204110233369.png)

![image-20210204111033384](/image-20210204111033384.png)



这个点击之后才出现下拉框

![image-20210204111218761](/image-20210204111218761.png)

这个链接地址就是视频所在url。

完善

useragent应该使用动态的

![image-20210204113550428](/image-20210204113550428.png)

点击之后这就是链接地址。

但是真正位置还需要加上一个前缀![image-20210204113705545](/image-20210204113705545.png)

其实也就是源码中的点击之后获得源码中获得的url。他request的源代码到这一层就结束了。

![image-20210204163556591](/image-20210204163556591.png)

![image-20210204184830538](/image-20210204184830538.png)

我们要请求的地址：https://f.video.weibocdn.com/Q9fBi7UMlx07K3B0OMWI010412013yjx0E010.mp4?label=mp4_hd&template=480x720.25.0&trans_finger=d8257cc71422c9ad30fe69ce9523c87b&ori=0&ps=1EO8O2oFB1ePdo&Expires=1612438106&ssig=s8dF79pY%2FR&KID=unistore,video

脚本中的编码

```
https%3A%2F%2Ff.video.weibocdn.com%2FQ9fBi7UMlx07K3B0OMWI010412013yjx0E010.mp4%3Flabel%3Dmp4_hd%26template%3D480x720.25.0%26trans_finger%3Dd8257cc71422c9ad30fe69ce9523c87b%26ori%3D0%26ps%3D1EO8O2oFB1ePdo%26Expires%3D1612438106%26ssig%3Ds8dF79pY%252FR%26KID%3Dunistore%2Cvideo
```

数据保存在这里面但是经过编码的。

%2F是/

%3A是：

%3F是？

%3D是=   

%26是&

%25是%

%2C是,  （逗号)

.（点）还是不变

下划线没有变

[URL编码参考](https://www.w3cschool.cn/htmltags/html-urlencode.html)

所以这就涉及对Url进行解码：

[编码解码代码实现](https://blog.csdn.net/qq_33876553/article/details/79730246)

所以我们接下来需要进行的是如何获得script里面对应的编码好的url。

参考b站那个代码

![image-20210204193250054](/image-20210204193250054.png)

![image-20210204214853369](/image-20210204214853369.png)

都是上面红框那一栏。

如何解析出来说一个json数据格式（**他妈的)**

第一次解析之后，json数据格式一看

![image-20210205093318276](/image-20210205093318276.png)



最终得到里面的html，自己加上html封装一下，得到

![image-20210205094849433](/image-20210205094849433.png)



![image-20210205095941673](/image-20210205095941673.png)

这里前面的想法

```python
html_data.text是前面requst.get返回的数据
# 下面的思路
找到对应的script下的标签，有不同script，但是发现视频那一栏的html在索引为2的那个script里面，然后去掉前后一些东西得到一个json字符串，使用Loads解析json最终得到里面的hml字符串。

html_data =html_data.text

# 正则表达式匹配
video_title= re.findall('<script charset="utf-8">FM.view\(([\d\D]*?)\)</script>',html_data)  # 返回得是一个列表
if(len(video_title)>=3):
    video_title = video_title[2]
    with open('第一次得到正则.html','w',encoding='utf-8') as f:
        # if(len(video_title)!=0):
        f.write(video_title)

    # 得到的是一个json格式的字符串
    json_scipt_data = json.loads(video_title)
    json_html_str= json_scipt_data['html']
    #  得到是html字符串
    # video\.weibocdn([\d\D]*?)unistore
    # https://f.video.weibocdn.com 参数  =unistore,video
    #  所以我们需要拼接一下然后转码，因为得到的字符串参数是没有编码的
else:
    print('没有匹配成功')
```

基于上面，经过测试发现，我直接全局正则匹配对应的视频URL链接。还稳定

推荐正则表达式测试使用得软件：没有广告：RegexMatchTracerCr

[下载位置](https://www.jb51.net/softs/34398.html#downintro2)

```python
re.findall('video\.weibocdn([\d\D]*?)unistore',contents)
```

结果又发现取得真正得url。

他前缀不一定一样，**脑壳痛**

![image-20210205105352787](/image-20210205105352787.png)

那我们就先搞微博开头得，你正常解码一对比，发现

<img src="/image-20210205110028455.png" alt="image-20210205110028455" style="zoom:50%;" />

你解析得直接点击原来发现服务器给你禁止了。

[python爬虫禁止访问解决办法](https://blog.csdn.net/u011808673/article/details/80609221)

换个ip，有效

![image-20210205110507360](/image-20210205110507360.png)

提取标题

![image-20210205140131771](/image-20210205140131771.png)

对于H3标签下，a和img数量不固定，但是还

开头，里面有个谁的微博视频那个标签**目前**发现是固定的。

正则表达式：

[python去掉标签及标签里面的内容](https://blog.csdn.net/Elimeny/article/details/89087442)

已经这个来划分。

`list_title_s(.*?)的微博视频`

![image-20210205141145035](/image-20210205141145035.png)

然后我再删除非汉字（文本的内容）

### 版本1.1总结

一定要带着cookie走，如何usergent换他里面的，不然会返回东西，但不是你想要的。

这里写完了：

![image-20210205151555989](/image-20210205151555989.png)

该py文件可以直接运行，只需要下载依赖库

因为我自己安装的是`anaconda+python3.7`再加上里面安装的库

<img src="/image-20210205152216818.png" alt="image-20210205152216818" style="zoom: 80%;" />

![image-20210205152302689](/image-20210205152302689.png)

没有使用selemium这种操作，里面的文件只是当时的一个思路，但实际中未使用。

**这是最原始的版本，能爬取视频**

但是需要优化的是：

```
1：动态从csv中获取相同IP，即使用代理池，ip全部保存在Ip_pool.csv中  
2：当过多访问ip导致被封时候，再动态获取ip
3：注意获得的视频地址和标题数目是否一样多
4：视频地址解析目前其实有问题，因为不一定都是https://f.video.weibocdn 开头的
4：保存该视频链接来源，这样打开数据就知道该视频哪里来的（这个暂时不需要，都写微博搞笑视频就行了）
5：对一些错误进行处理
6：进行实现多页面翻页
6：正则表达式需要优化
7：多线程
8：动态使用user-agent
9: 架构一点，多封装
```

对于Ip的csv文件的处理

```python
# 第一点优化实现，顺序使用，如果该ip被屏蔽之后就依次调用第二个，这样就不会因为随机数重复使用csv中被屏蔽的ip
import pandas as pd 
#  这里使用读取csv来获得ip，不用以下方式就注释掉，直接用上面的就可以
df = pd.read_csv("ip_pool.csv")
# #  这里直接变成数组格式
df_list=df.values.tolist()
# 我没有必要取随机数，如果这一个不行我直接取下一个
# 这个索引是从0开始
csv_ip_local = 0   # csv中用到第几个Ip了
proxies = {
            # type :ip   行列定位
            df_list[csv_ip_local][0]: df_list[csv_ip_local][1]
        }

```

```
// 但这样会存在一个问题，因为从快代理的ip我以前使用过被微博禁了
思路：获取Ip的时候测试是否有效不仅经过`http://httpbin.org/get`,还经过微博网站试探一下，如果可以就加入csv里面。
```

出现这个问题

![image-20210207084936318](/image-20210207084936318.png)

连续几个之后就不能用了。

我觉得是因为

![image-20210207094030297](/image-20210207094030297.png)



对于`WeiBo1.3.py`是对正则表达式的完全重构。针对

![image-20210207084936318](/image-20210207084936318.png)



这里出现不可用的情况是因为有些视频也是别人搬运的，在别的网站上

`WeiBo1.3.py`之前的都是只获得https://f.video.weibocdn.com :这种开头类型的，但是对于从秒拍视频这种搬运的`https://n1cdn.miaopai.com/stream/`就无法获取，至于为什么获取的视频和得到的标题能一一对于，是因为你获得标题的正则表达式：![image-20210207100040793](/image-20210207100040793.png)

后缀是：的微博视频    因为搬运的后缀：比如从秒拍视频的来的：![image-20210207100134865](/image-20210207100134865.png)

所以前面把外面视频网址去掉，视频标题把外面视频标题去掉，所以正好一一对应。

**解决办法**

把视频的栏目这一栏放到一个整体来读取

![image-20210207100523480](/image-20210207100523480.png)

你会发现`UG_list_v2 clearfix`是每个视频的标题的class。

所以得到的视频的标题：`UG_list_v2 clearfix([\d\D]*?)视频`。然后再去解析。

遇见问题：

![image-20210207113623332](/image-20210207113623332.png)

上面这是ip被封了。

### 版本1.5添加：

```
优化：
	1：ip和useragent都取随机数，如果一个ip重复读取更容易被封
	2：被封的ip需不需要再删掉，全部ip被封在调用爬取匿名代理的函数（这个idea方法暂定）
```

**1：useragent都取随机数**

[参考](https://www.cnblogs.com/yunlongaimeng/p/10682090.html)

​	weibo1.4.py经常ip被封只能读取下一个

实现：

​	使用fake-useragent

```
 pip install fake-useragent
```

![image-20210301100243614](/image-20210301100243614.png)

**2：ip取随机数**

```python
# 重新获得随机IP    
def get_randon_ip():
    if(list_ip_len):
        csv_ip_local = random.randint(0,list_ip_len-1) # csv中用到第几个Ip了
    else:
        csv_ip_local=0
        print('已经没有可用的IP地址了')  # 后面可以在这里选择是否重新爬取IP形成代理池
    new_proxies = {
        # type :ip   行列定位
        df_list[csv_ip_local][0]: df_list[csv_ip_local][1]
    }
    return new_proxies
```



# b站（selenium）

## 百度测试（手动验证码）

[获得验证码](https://blog.csdn.net/qq_36853469/article/details/100579355)

模拟点击图片？？？？因为每个验证码点击不同，如何中间自己手工测试点击顺序和图片位置呢？？？

自己测量之后，使用input暂停程序，等自己测量完之后输入数据，程序接着执行。

![image-20210309163711409](/image-20210309163711409.png)

## 扫码登录

​	新的思路：人工扫码登录获取验cookie，然后利用cookie登录。

[大麦网](https://blog.csdn.net/weixin_43821172/article/details/105199481)

[淘宝网](https://blog.csdn.net/weixin_45774059/article/details/107308844)

## QQ登录

[qq登录](https://blog.csdn.net/LRXCK/article/details/113807465)

## 人工的登录得到cookie（已实现）

[人工先登录，获得cookie保存到本地](https://blog.csdn.net/qq_42358311/article/details/108257486?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-1&spm=1001.2101.3001.4242)

登录之后我们点击创造中心，这时候需要跳转到新页面，我们需要考虑句柄。

每次才进入页面需要点击跳过（自动测试打开的网页有，自己打开的时候基本没有），然后才能点击上传视频等操作。

点击上传本地文件

[总的方法：代码用不了](https://blog.csdn.net/huilan_same/article/details/52439546)

因为上传本地文件使用的是非input标签的，python3.7没有sendkeys库，我们使用PyUserInput。

[安装教程](https://blog.csdn.net/dianmomanxue/article/details/95044676)

后期调优化：

​	使用隐式等待：driver.implicitly_wait(8)



但是我们直接点击投稿就行了，前面反而麻烦了。





### 异常处理

**一：**

![image-20210311191930720](/image-20210311191930720.png)



这个使用time.sleep()试一下，可能页面元素还没加载进来

**二：**对上传文件元素无法定位问题

​	![image-20210311193352882](/image-20210311193352882.png)

因为存在iframe。[解决办法](https://blog.csdn.net/huilan_same/article/details/52200586)

这相当于跳转到另外一个html中去了。如果需要操作外层则需要跳出来。

**三：页面还未加载**

![image-20210314000158181](/image-20210314000158181.png)

解决办法： time.sleep()

**四：“搞笑视频”  这个按钮定位不行**

![image-20210314190816499](/image-20210314190816499.png)

![image-20210314190825089](/image-20210314190825089.png)

这不一定是第三个位置。

解决办法：更换定位方式

![image-20210314201042291](/image-20210314201042291.png)

为了兼容 搞笑视频 标签，我们直接选择第三个标签，（但是有时候第三个标签名字不叫搞笑视频）

五：服务器没有响应

**weibo1.7.py**

![image-20210314210935973](/image-20210314210935973.png)





这么写文字的时候：问好需要小写，文字前后空格需要忽略  使用stricp方法

然后你使用PyUserInput在输入框里面无法输入中文。

[解决中文问题](https://blog.csdn.net/killvirus007/article/details/89497833?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-15.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-15.control)

上面方法还是不能解决中文的问题，可以这样：获取微博视频的时候使用编号，然后使用一个csv文件来保存每个mp4的名字，甚至是具体的链接。

对于上传文件之后选择“转载按钮“没反应。

尝试一：页面滑动一下看看按钮是否出现在页面中。

<img src="/image-20210312140328236.png" alt="image-20210312140328236" style="zoom:50%;" />

[下拉滚条](https://blog.csdn.net/lyl_7310/article/details/78532628)

当xpath找不到的时候，可以将定位在里面找一找。crtl+F

**1.3.py版本**

![image-20210312152738686](/image-20210312152738686.png)

```
实现操作：
		getcookie.py 运行的时候自己手动登录b站，然后该程序保存cookie到cookie.txt中。
		运行1.3.py 从带cookie直接登录b站，模拟使用投稿，投视频
未实现功能
		1：因为上传文件无法输入中文字符，将视频变成序号，标题名字还是固定
		2：未实现多文件重复上传
		3：time.sleep()和其他等待操作还需要优化。
```

**1.4版本**

```
将实现
	1：视频需要和对于的csv文件（保存视频名字，所在链接）
	2：实现多文件上传
	3：每隔一定时间上传
```

这我们需要将`weibo1.5.py`修改`weibo1.6.py`为了将视频分类为需要和csv文件。

如果手动的上传你使用`WeiBo1.5.py`下载视频，如果自动配合b站模拟登录，你使用`WeiBo1..py`下载。

视频和名字对应csv文件`source_introd.csv`

发现chrome无法识别表情符号。

[过滤表情包](https://blog.csdn.net/qq_37275405/article/details/100541462)

在`WeiBo1.6.py`中，我们需要没有名称得视频去除。

![image-20210313005006523](/image-20210313005006523.png)

接着更新，如果产生的获得重复的视频怎么办？？得到视频名字的时候判断是以前已经获得过，获得过的话不重新获得。意思使用一个csv文件保存起来。（以后有希望的话可以用数据库）

`WeiBo.17.py`**增加重复视频判断**

[pandas进行输入筛选](https://www.cnblogs.com/gaodi2345/p/11756513.html)

```
df.loc[df['name'].str.contains('丸子头还可以这么用')]
```

pandas添加新行和新列

[添加](https://www.jb51.net/article/187286.htm)

# 使用方式

```
weibo文件夹	
	weibo1.3.py :获取一页视频（和名字一起）
	weibo1.5.py ： 直接获得视频，视频下的就是视频名字
	weibo1.6.py :  获得视频，下面是序号，
								对于的序号和视频名字是在source_introd.csv中
	weibo1.7.py:   相对于1.6.py 版本，添加了alreday_upload.csv 来							    保存下载的记录，如果前面下载过就没必要再下载该视频
								 对视频名字：去表情符号，前后字符串，&nbsp;
cookie文件夹
	getcookie.py   :程序运行之后你手动登录，后台会获得cookie
	cookie1.6.py   :目前最新的自动登录上传视频
	cookie1.7.py   :优化一些功能
								
```

![image-20210315104123556](/image-20210315104123556.png)

注意上面开口，如果中间上传出bug了，可以 手动 选择下一个从具体序号接着上传

### `cookie1.7.py`版本 

​	配合`WeiBo1.7.py` 版本

```
相对于`cookie1.6.py`进行了一些优化
1：发现不用js版本滑动窗口也可以获得到页面元素，因此我们在这个版本中去掉
2：time.sleep()时间的适当减少，减小上传等待时间。
3：去掉标题名字中自带的序号，然后再输入标题名字
4: 输入具体的视频链接地址
```

**去掉视频默认标题**

​	[我选的是里面方法二](https://blog.csdn.net/sun_977759/article/details/108731881)

**改变文件上传方法：使用pywin32方法 **

```
pip install pywin32
```

[pywin32出现的问题](https://stackoverflow.com/questions/58989806/anaconda-terminal-error-pywin32-bootstrap)

`WeiBo1.8.py`

​	配合`cookie1.8.py`

```
上面`WeiBo1.8.py` 和`cookie1.8.py`所做的优化

cookie1.8.y
1: 直接访问投稿页面，没必要进入主界面再进入投稿界面
2：time.sleep()时间适当减少
3：与weibo1.8.py配合（之前的版本不行），实现标题上传为具体地址，避免写		死

WeiBo1.8.py
1:在 source_introd.csv 和 already_uload.csv 中保存了对外的url地   址，这样在标题栏目就不会单纯写 微博搞笑视频的地址了  避免写死
```

`cookie.1.9.py` 和 `WeiBo1.9.py` 版本

```
`WeiBo1.9.py`
1：视频加上水印和边框，然后保存在 combine_video 文件夹中
```

具体可以看`utils/CombineVideo.py`

关于第二个拼接的视频出现花屏的情况

[最终解决办法](https://www.codenong.com/cs106054516/)

[花屏处理](https://blog.csdn.net/ucsheep/article/details/84630800)

[moviepy使用大全](https://www.cnblogs.com/traditional/p/11144123.html)

尺寸差别太大会产生很大的形变

我觉得没必要一定是两个视频拼接，可以打水印，或者加边框。

`WeiBo1.9.py`

```
1：前面参考  最终解决办法 我们在组成视频的时他他自动改变分辨率，这样我们测试出来的视频拼接没有出现花屏
	具体方法：utils/combinevideo.py 中
2：基于前面第一点：我们将合并的视频名字取第一个，这样累计，意思就是source_introd.csv 更改，但是 alread_csv 不更改 这是下取整

发现的几点错误
1：Url_data = re.findall('UG_list_v2 clearfix([\d\D]*?)noopener noreferrer',html_data) 修改
2：source-introd 一旦出现错误里面就没有数据，但是前面的视频已经下载，因此代码操作csv 改成already-upload.csv 读写方式
```

1：Url_data = re.findall('UG_list_v2 clearfix([\d\D]*?)noopener noreferrer',html_data)   正则有时候报错

![image-20210318192024148](/image-20210318192024148.png)

noopener noreferrer 在右边第一个箭头就出现了，相当于提前出现，这样子无法得到这个视频的名字，我们的做法就是直接忽略掉。

2：source-introd.csv 改成read模式，里面向行添加数据.

但是我们需要每次读直接重写。

发现组合的顺序 ： 1 10  11 12  这样子的，所有我们需要改变默认排序。

[改变排序](https://www.jianshu.com/p/98a0c091c4bf)

**出现的严重问题**

安装前面`WeiBo1.9.py`来看，两个视频组合起来的大小比正常组合起来大10倍都有可能。

找到的答案：[码率，帧率等之间的关系](https://blog.csdn.net/pc9319/article/details/79621352)

我把码率这个参数去掉变成默认之后

![image-20210319163741623](/image-20210319163741623.png)

上图左边是加上码率，右边是不加上码率的。

但是我肉眼看了一下好像差不了太多。因此还是去掉吧





























# youtube视频爬取

https://www.youtube.com/results?search_query=%E7%88%86%E7%AC%91%E8%A7%86%E9%A2%91

避过大IP，爬取的时候如果是又名字为抖音两个字的就屏蔽。

![image-20210314150151853](/image-20210314150151853.png)

**对已经上传过视频的名字保存起来。**避免重复上传

暂时搁置

# 微博视频栏目爬取

https://weibo.com/tv/billboard/4418219809678869

